---
title: "Homework 4 Solutions"
subtitle: "Advanced Statistical Computing"
author: "Alexander Durbin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(boot)
```

# 150,2.75,-0.15

# Problem 1
## Part A

The following code contains all function declarations.

```{r}

powers1 <- function(x, dg)
{
  pw <- matrix(x, nrow=length(x))
  prod <- x ## current product
  for(i in 2:dg) {
    prod <- prod * x
    pw <- cbind(pw, prod)
  }
  return(pw)
}

powers2 <- function(x, dg)
{
  pw <- matrix(nrow=length(x), ncol=dg)
  prod <- x ## current product
  pw[,1] <- prod
  for(i in 2:dg) {
    prod <- prod * x
    pw[,i] <- prod
  }
}

powers3 <- function(x, dg) outer(x, 1:dg,"^")

powers4 <- function(x, dg) 
{
  repx <- matrix(rep(x, dg), nrow=length(x))
  return(t(apply(repx, 1, cumprod)))
}

x <- runif(10000000)
dg <- 16

```

`powers3` provides the shortest syntax for computing the powers of a vector. It uses the `outer` function in `R` with the exponentiation operator. It will loop through all of the values in `dg` and compute `x` raised to that power. There is certainly some form of copying involved here.

`powers4` tries to rectify the copying issue by preallocating storage in `repx` with copies of `x` itself. It then uses the `cumprod` function inside the `apply` statement to perform the exponentiation. The hope is that the `apply` statement can make up speed over using a `for` loop. However, we have shown in class that the `apply` family of functions can be slow in some cases and beaten by regualr `for` loops. This operation could be sped up by using the `parallel` package and a `parapply` statement instead.

Both of these functions elect for cleaner syntax than the original functions.

## Part B

The following code provides the runtimes for each of the above functions twice.

```{r}

system.time(powers1(x, dg))
system.time(powers1(x, dg))

system.time(powers2(x, dg))
system.time(powers2(x, dg))

system.time(powers3(x, dg))
system.time(powers3(x, dg))

system.time(powers4(x, dg))
system.time(powers4(x, dg))

```

We see that `powers2` is by far the fastest. `powers1` is about twice as slow as `powers1`. `powers3` is about four times slower than `powers1` and `powers4` is by far the slowest implimentation.

## Part C

The next code chunks will profile `powers3` and `powers4`. After each run, we remove `tmp` and use the `gc()` command to return memory back to the operating system in order to better identify CPU and memory usage differences. When knitting this document, profile statements have more noise from the `knitr` processes. To remove this noise, we run the following chunks in the console with no applications besides RStudio and a terminal open.

The following code profiles `powers3`.

```{r, warning=FALSE}

Rprof(memory.profiling = TRUE)
tmp <- powers3(x, dg)
Rprof(NULL)
summaryRprof()
format(object.size(tmp), units = 'Gb')
rm(tmp)
gc()

```

We see that almost all of the computational effort is spent inside the `outer` function. Also checking `htop` during execution of the above commands in the console, we see CPU usage spike to 100% as well as more than double memory usage from 2.33 Gb to 5.37 Gb, though some of this is certainly due to the returned object size. There are no specific subroutines or functions within `outer` that we can attribute this slow runtime to.

The following code profiles `powers4`.

```{r}

Rprof(memory.profiling = TRUE)
tmp <- powers4(x, dg)
Rprof(NULL)
summaryRprof()
format(object.size(tmp), units = 'Gb')
rm(tmp)
gc()

```

Using `htop` during this runtime, we see that memory usage increase from 2.3Gb to over 9Gb. `powers4` is an extremely memory intensive process. CPU usage is also at 100% during execution. There are multiple subroutines within `powers4` that we can arttribute this bottleneck to. As suggest above, the `apply` statement is the most intensive process. Surprisingly, the transpose statement is also intensive. There are also several subroutines that are also computationally intensive, though they are subroutines of the above two statements. The `apply` statement includes the subroutines `FUN`, `aperm.default`, `array`, and `lengths`, of which `FUN` and `aperm.default` are most intensive. The `tranpose` statement includes `matrix`, `array`, and `lengths` subroutines. This operation can be sped up using the `parallel` package with a `parApply` statement, however this may not solve the memory issues.

# Problem 2

To accomodate other distributions, we need only change the sampling procedure. We use a simple function that takes two arguments, the distributions of both Annie and Sam. We include a couple safety check to ensure that the distributions are the same length.

```{r}

as_prob <- function(dista, dists){
  if(length(dista) != length(dists)){
    stop('Sample size must be the same for both Annie and Sam.')
  }
  
  S <- dista
  A <- dists
  # probability Annie arrives later than Sam
  prob <- mean(A < S)
  cat("\nThe probability Annie arrives after Sam is ", prob)
  #expected differences
  diff <- A - S
  cat("\nThe expected difference between arrival times is ", mean(abs(diff)), "\n")
}

as_prob(1:10, rnorm(10,10,1))
as_prob(runif(1000, 10, 11.5), runif(1000, 10, 12))
as_prob(rnorm(1000, 10.5, 1), rnorm(1000, 11, 1.5))
as_prob(1:9, 1:10)

```

# Problem 3

The bootstrap linear regression example from class is the following code.

```{r}

## bootstrapped linear regression
n <- 200
X <- 1/cbind(rt(n, df=1), rt(n, df=1), rt(n,df=1))
beta <- c(1,2,3,0)
Y <- beta[1] + X %*% beta[-1] + rnorm(100, sd=3)
fit <- lm(Y~X)
coef(fit)  ## try/not in slides
summary(fit)$cov.unscaled

## now take the bootstrap samples
B <- 10000
beta.hat.boot <- matrix(NA, nrow=B, ncol=length(beta))
for(b in 1:B) {
  indices <- sample(1:nrow(X), nrow(X), replace=TRUE)
  beta.hat.boot[b,] <- coef(lm(Y[indices]~X[indices,]))
}


```

Our code using the `boot` library is as follows. We need to define a 'statistic' in the `boot` in order to get the regression coefficients from the bootstrap samples.

```{r}

## 'statistic' for boot call
reg_coef <- function(data, indices){
  fit <- lm(Y ~ ., data = data[indices, ])
  return(coef(fit))
}

## boot call
df <- data.frame(Y = Y, X = X)
bs_reg <- boot(data = df, statistic = reg_coef, R = B)

```

We next compare the covariance matrices. 

```{r}

## from fit
summary(fit)$cov.unscaled
## manual boot samples
cov(beta.hat.boot)
## boot library samples
cov(bs_reg$t)

```

Comparing the covariance matrices, we see that the covariance matrices of the bootstrap methods are very different from the truth. Comparing the manual and library bootstrap procedures, the covariances of $\beta_3, \beta_1$, $\beta_4, \beta_1$, $\beta_4, \beta_2$, and $\beta_4, \beta_3$ are also very different.

We modify the plot statement used in class to get side-by-side histograms to better compare the manual versus library bootstrapping.

```{r}

## comparing marginals
par(mfrow=c(1,2))
for(i in 1:4) {
  m <- coef(fit)[i]
  s1 <- sqrt(cov(beta.hat.boot)[i,i])
  s2 <- sqrt(cov(bs_reg$t)[i,i])
  x1 <- m + seq(-4*s1, 4*s1, length=1000)
  x2 <- m + seq(-4*s2, 4*s2, length=1000)
  xlim1 <- range(c(x1, beta.hat.boot[,i]))
  xlim2 <- range(c(x2, bs_reg$t[,i]))
  hist(beta.hat.boot[,i], freq=FALSE, xlim=xlim1,
       xlab=i, main="manual", breaks=20)
  lines(x1, dnorm(x1, m, s1), col=2, lwd=2)
  hist(bs_reg$t[,i], freq=FALSE, xlim=xlim2,
       xlab=i, main="library", breaks=20)
  lines(x2, dnorm(x2, m, s2), col=2, lwd=2)
}

```

Comparing the marginal distributions of the coefficients, we see that the manual and library-based bootrap distributions are nearly identical to each other. However, the $\beta_1$ coefficients for both methods do not follow the theoretical distribution of the coefficient. This is due to the resampling procedure.

# Problem 4

We generate the data exactly as in class. We then obtain bootstrap samples before using `smooth.spline` with cross-validation to make predictions. We store all of the bootstrap predictions before taking the means and quantiles for plotting. For this code chunk we have to turn off warnings, as `smooth.spline` throws warnings for non-unique samples during cross-validation.

```{r, warning=FALSE}


## non-linear data
X <- seq(0,10,length=50)
Ytrue <- (sin(pi*X/5) + 0.2*cos(4*pi*X/5))
Y <- Ytrue + rnorm(length(Ytrue), sd=0.1)

## predictive locations
XX <- seq(0,10,length=199)

## bootstrap iterations
B <- 1000

## storage for predictions
sto <- matrix(NA, nrow = B, ncol = length(XX))

## bootstrap procedure
for(i in 1:B){
  indices <- sample(1:length(X), size = length(X), replace = TRUE)
  xboot <- X[indices]
  yboot <- Y[indices]
  fit <- smooth.spline(x = xboot, y = yboot, cv = TRUE)
  preds <- predict(fit, XX)
  sto[i, ] <- preds$y
}

## get means and quantiles
pred_mean <- apply(sto, 2, mean)
pred_low <- apply(sto, 2, function(col) quantile(col, probs = c(0.05)))
pred_high <- apply(sto, 2, function(col) quantile(col, probs = c(0.95)))

## plots
plot(X,Y, main = "Bootstrapped Spline With Cross-Validation")
lines(XX, pred_mean, col = "blue", type = "l", lwd = 2)
lines(XX, pred_low, col = "red", lty = 6, lwd = 1)
lines(XX, pred_high, col = "red", lty = 6, lwd = 1)

```